{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "enviroment: flappy bird\n",
    "    input: 5 states, reward: -5 die, +0.05 survive, +5 pass pipe\n",
    "    output: 2 actions\n",
    "\n",
    "framework: tf.keras \n",
    "model: 2 fully connected neural networks\n",
    "    layers:\n",
    "        Actor: 5 input | 64 relu | 64 relu | softmax -> policy\n",
    "        Critic: 5 input | 64 relu | 64 relu | linear -> value (value[action] is probs of action)\n",
    "    params: state s, action a, reward R, value V \n",
    "    hyperparams: discount 0.999, learning rate 0.0003 optimizer adam, clipping epsilon: 0.2 (best)\n",
    "    algorithm: PPO\n",
    "        bellman Q = R + discount * Q', advantage A = Q - V, ratio = new_probs/old_probs, clipped ratio\n",
    "        Actor loss: mean (min (ratio policy * advantage), (clipped ratio * advantage))\n",
    "        Critic loss: mean square (advantage)\n",
    "\n",
    "result: performing exactly like A2C... or at least no significantly changes\n",
    "    test:\n",
    "    ep 250: model score first pipe for the first time\n",
    "    ep 450: model start score first pipe more often, peaked 2 score\n",
    "    ep 1000: model score first pipe most the time, trying to reduce... (well it exactly same)\n",
    "    ep ~5000: trending score are 1-4 with a few 5s, peaked 9\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pygame\n",
    "import sys\n",
    "from enviroment import FlappyBird\n",
    "from modelPPO import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FlappyBird()\n",
    "scores, rewards = [], []\n",
    "\n",
    "model = PPO()\n",
    "model.actor = tf.keras.models.load_model('models/PPOactor.keras')\n",
    "model.critic = tf.keras.models.load_model('models/PPOcritic.keras')\n",
    "\n",
    "for episode in range(10000):\n",
    "    model.training_loop(env, 10000)\n",
    "    scores.append(model.score)\n",
    "    rewards.append(model.reward)\n",
    "    display.clear_output(wait=True)\n",
    "    print(f\"Episode {episode}:\")\n",
    "    print(f\"current score: {model.score:.2f}, highest score: {max(scores):.2f}, avg score: {np.mean(scores):.2f}\")\n",
    "    print(f\"current reward: {model.reward:.2f}, highest reward: {max(rewards):.2f}, avg reward: {np.mean(rewards):.2f}\")\n",
    "    print(f\"actor loss: {model.actor_loss:.4f}, critic loss: {model.critic_loss:.4f}\")\n",
    "pygame.quit()\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward EMA\n",
    "previous_reward = 0\n",
    "R_EMA = []\n",
    "factor = 2 / (1+50)\n",
    "for reward in rewards:\n",
    "    if previous_reward != 0:\n",
    "        new_reward = previous_reward * (1 - factor) + reward * factor\n",
    "    else:\n",
    "        new_reward = reward\n",
    "    previous_reward = new_reward\n",
    "    \n",
    "    R_EMA.append(new_reward)\n",
    "\n",
    "plt.plot(list(range(len(rewards))), rewards, alpha=0.5, label=\"Reward\")\n",
    "plt.plot(list(range(len(R_EMA))), R_EMA, label=\"Exponential Moving Average\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_counts = Counter(scores)\n",
    "trending = score_counts.most_common()\n",
    "print(\"Trending Scores:\")\n",
    "for rank, (score, count) in enumerate(trending, 1):\n",
    "    print(f\"{rank}. Score: {score} | Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.actor.save('models/PPOactor.keras')\n",
    "model.critic.save('models/PPOcritic.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
