{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "enviroment: flappy bird\n",
    "    input: 5 states, reward: -5 die, +0.05 survive, +5 pass pipe\n",
    "    output: 2 actions\n",
    "\n",
    "framework: tf.keras \n",
    "model: 2 fully connected neural networks\n",
    "    layers:\n",
    "        Actor: 5 input | 64 relu | 64 relu | softmax -> policy\n",
    "        Critic: 5 input | 64 relu | 64 relu | linear -> value\n",
    "    params: state s, action a, reward R, value V\n",
    "    hyperparams: discount 0.999, learning rate 0.001 and 0.0003 optimizer adam\n",
    "    algorithm: A2C\n",
    "        bellman Q = R + discount * Q', advantage A = Q - V, log policy\n",
    "        Actor loss: mean (log policy * advantage)\n",
    "        Critic loss: mean square (advantage)\n",
    "\n",
    "result: learning-average and highest score/reward slowly increase over time\n",
    "    test:\n",
    "    ep400: actor loss begin to convergence around +-1, critic loss start to converges slowly: around 10-20 = able to cross pipe smoothly\n",
    "    ep1000: model score first pipe most the time, trying to reduce critic loss at second pipe and peaked 3 score\n",
    "    ep3000: model score 2 pipe most the time, trying to reduce critic loss at third pipe and peaked 4 score\n",
    "    ep13000: 1-4 are trending score, fewer 0s and more 5s and 6s, peak at 11. Critic increase at pipe 1, 2 but still pass anyway. Other pipes loss reducing\n",
    "    ep...: model stop learning. Trending score respectively 2 3 4 1 5, peak at 12, mva varies around 3 score\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pygame\n",
    "import sys\n",
    "from enviroment import FlappyBird\n",
    "from modelA2C import A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FlappyBird()\n",
    "scores, rewards = [], []\n",
    "\n",
    "model = A2C()\n",
    "model.actor = tf.keras.models.load_model('models/A2Cactor.keras')\n",
    "model.critic = tf.keras.models.load_model('models/A2Ccritic.keras')\n",
    "\n",
    "for episode in range(50000):\n",
    "    model.trainingLoop(env, 10000)\n",
    "    scores.append(model.score)\n",
    "    rewards.append(model.reward)\n",
    "    display.clear_output(wait=True)\n",
    "    print(f\"Episode {episode}:\")\n",
    "    print(f\"current score: {model.score:.2f}, highest score: {max(scores):.2f}, avg score: {np.mean(scores):.2f}\")\n",
    "    print(f\"current reward: {model.reward:.2f}, highest reward: {max(rewards):.2f}, avg reward: {np.mean(rewards):.2f}\")\n",
    "    print(f\"actor loss: {model.actorLoss:.4f}, critic loss: {model.criticLoss:.4f}\")\n",
    "pygame.quit()\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward EMA\n",
    "previous_reward = 0\n",
    "R_EMA = []\n",
    "factor = 2 / (1+50)\n",
    "for reward in rewards:\n",
    "    if previous_reward != 0:\n",
    "        new_reward = previous_reward * (1 - factor) + reward * factor\n",
    "    else:\n",
    "        new_reward = reward\n",
    "    previous_reward = new_reward\n",
    "    \n",
    "    R_EMA.append(new_reward)\n",
    "\n",
    "plt.plot(list(range(len(rewards))), rewards, alpha=0.5, label=\"Reward\")\n",
    "plt.plot(list(range(len(R_EMA))), R_EMA, label=\"Exponential Moving Average\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score EMA\n",
    "previous_score = 0\n",
    "S_EMA = []\n",
    "factor = 2 / (1 + 50)\n",
    "\n",
    "for score in scores:\n",
    "    if previous_score != 0:\n",
    "        new_score = previous_score * (1 - factor) + score * factor\n",
    "    else:\n",
    "        new_score = score\n",
    "    previous_score = new_score\n",
    "\n",
    "    S_EMA.append(new_score)\n",
    "\n",
    "plt.plot(list(range(len(scores))), scores, alpha=0.5, label=\"Scores\")\n",
    "plt.plot(list(range(len(S_EMA))), S_EMA, label=\"Exponential Moving Average\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_counts = Counter(scores)\n",
    "trending = score_counts.most_common()\n",
    "print(\"Trending Scores:\")\n",
    "for rank, (score, count) in enumerate(trending, 1):\n",
    "    print(f\"{rank}. Score: {score} | Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.actor.save('models/A2Cactor.keras')\n",
    "model.critic.save('models/A2Ccritic.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
